{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "nb_word_class = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(s):\n",
    "    c = s.lower().strip()\n",
    "    return re.sub('[^a-z ]', '', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_word_class(tag):\n",
    "    if tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return 1 #'adjective'\n",
    "    if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return 2 #'noun'\n",
    "    if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return 3 #'verb'\n",
    "    if tag in ['CC', 'IN']:\n",
    "        return 4 #'link'\n",
    "    return 0 #'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_tagging(sentence):\n",
    "    translation = list()\n",
    "    for word, tag in sentence:\n",
    "        translation.append((word, translate_word_class(tag)))\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_encoding(sentence):\n",
    "    encoded = list()\n",
    "    for word, tag in sentence:\n",
    "        encodedInt = one_hot(word,30000)[0]\n",
    "        encoded.append([encodedInt,tag])\n",
    "    return encoded        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "END = 5\n",
    "def sentence_labeling(sentence):\n",
    "    labels = list()\n",
    "    for word, tag in sentence[1:]:\n",
    "        labels.append(tag)\n",
    "    labels.append(END)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_words(sentences):\n",
    "    tagged_words = list()\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words.append(nltk.pos_tag(words))\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Ymatrix(numbers):\n",
    "    matrix = np.zeros((len(numbers),1))\n",
    "    i = 0\n",
    "    for number in numbers:\n",
    "        matrix[i] = number\n",
    "        i=i+1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Xmatrix(numbers,size,var_index):\n",
    "    matrix = np.zeros((size,1))\n",
    "    sub_matrix = np.zeros((1))\n",
    "    i = 0\n",
    "    for number in numbers:\n",
    "        sub_matrix[0] = number[var_index]\n",
    "        matrix[i][0] = sub_matrix\n",
    "        i=i+1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sentences = list()\n",
    "with open('./data_set/training_set70.txt') as train:\n",
    "    for line in train:\n",
    "        train_sentences.append(clean_sentence(line))\n",
    "\n",
    "test_sentences = list()\n",
    "with open('./data_set/test_set25.txt') as train:\n",
    "    for line in train:\n",
    "        test_sentences.append(clean_sentence(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sentences = tag_words(train_sentences)\n",
    "tagged_sentences_test = tag_words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('sam', 'JJ'), ('didnt', 'NNS'), ('like', 'IN'), ('elections', 'NNS')],\n",
       " [('dick', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('sam', 'NN'),\n",
       "  ('were', 'VBD'),\n",
       "  ('disappointed', 'JJ')],\n",
       " [('where', 'WRB'), ('is', 'VBZ'), ('my', 'PRP$'), ('apple', 'NN')],\n",
       " [('yes', 'RB'), ('thats', 'NNS'), ('correct', 'VBP')],\n",
       " [('no', 'DT'), ('its', 'PRP$'), ('wrong', 'NN')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[:5]\n",
    "tagged_sentences_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tagged_sentences = list(map(map_tagging, tagged_sentences))\n",
    "test_tagged_sentences = list(map(map_tagging, tagged_sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dick', 2), ('had', 3), ('a', 0), ('great', 1), ('time', 2), ('at', 4), ('the', 0), ('park', 2)]\n"
     ]
    }
   ],
   "source": [
    "my_tagged_sentences[:5]\n",
    "my_tagged_sentences.reverse()\n",
    "print(my_tagged_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = flatten(map(map_encoding, my_tagged_sentences))\n",
    "test = flatten(map(map_encoding, test_tagged_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 2.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 4.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 4.],\n",
       "       [ 2.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 3.],\n",
       "       [ 1.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 4.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 4.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 0.],\n",
       "       [ 5.],\n",
       "       [ 3.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 4.],\n",
       "       [ 0.],\n",
       "       [ 2.],\n",
       "       [ 5.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_Ymatrix(flatten(map(sentence_labeling,my_tagged_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = create_Xmatrix(x,len(x),0)\n",
    "y_train = to_categorical(flatten(map(sentence_labeling,my_tagged_sentences)),nb_word_class)\n",
    "x_test = create_Xmatrix(test,len(test),0)\n",
    "y_test = to_categorical(flatten(map(sentence_labeling,test_tagged_sentences)),nb_word_class)\n",
    "\n",
    "#print('X_train shape:', x_train.shape)\n",
    "#print('X_test shape:', x_test.shape)\n",
    "#print('y_train shape:', y_train.shape)\n",
    "#print('y_test shape:', y_test.shape)\n",
    "#print(y_train)\n",
    "#print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 132 samples\n",
      "Epoch 1/20\n",
      "328/328 [==============================] - 1s - loss: 1.7923 - acc: 0.1524 - val_loss: 1.7907 - val_acc: 0.2424\n",
      "Epoch 2/20\n",
      "328/328 [==============================] - 0s - loss: 1.7907 - acc: 0.2713 - val_loss: 1.7895 - val_acc: 0.3258\n",
      "Epoch 3/20\n",
      "328/328 [==============================] - 0s - loss: 1.7887 - acc: 0.3140 - val_loss: 1.7882 - val_acc: 0.3864\n",
      "Epoch 4/20\n",
      "328/328 [==============================] - 0s - loss: 1.7865 - acc: 0.3537 - val_loss: 1.7868 - val_acc: 0.4318\n",
      "Epoch 5/20\n",
      "328/328 [==============================] - 0s - loss: 1.7850 - acc: 0.4604 - val_loss: 1.7854 - val_acc: 0.5909\n",
      "Epoch 6/20\n",
      "328/328 [==============================] - 0s - loss: 1.7828 - acc: 0.5640 - val_loss: 1.7839 - val_acc: 0.6439\n",
      "Epoch 7/20\n",
      "328/328 [==============================] - 0s - loss: 1.7805 - acc: 0.6372 - val_loss: 1.7825 - val_acc: 0.6742\n",
      "Epoch 8/20\n",
      "328/328 [==============================] - 0s - loss: 1.7774 - acc: 0.7043 - val_loss: 1.7809 - val_acc: 0.6818\n",
      "Epoch 9/20\n",
      "328/328 [==============================] - 0s - loss: 1.7777 - acc: 0.6402 - val_loss: 1.7794 - val_acc: 0.6742\n",
      "Epoch 10/20\n",
      "328/328 [==============================] - 0s - loss: 1.7756 - acc: 0.6646 - val_loss: 1.7779 - val_acc: 0.6742\n",
      "Epoch 11/20\n",
      "328/328 [==============================] - 0s - loss: 1.7716 - acc: 0.7591 - val_loss: 1.7763 - val_acc: 0.6742\n",
      "Epoch 12/20\n",
      "328/328 [==============================] - 0s - loss: 1.7701 - acc: 0.7561 - val_loss: 1.7747 - val_acc: 0.6742\n",
      "Epoch 13/20\n",
      "328/328 [==============================] - 0s - loss: 1.7668 - acc: 0.7439 - val_loss: 1.7731 - val_acc: 0.6742\n",
      "Epoch 14/20\n",
      "328/328 [==============================] - 0s - loss: 1.7642 - acc: 0.7530 - val_loss: 1.7714 - val_acc: 0.6742\n",
      "Epoch 15/20\n",
      "328/328 [==============================] - 0s - loss: 1.7619 - acc: 0.7378 - val_loss: 1.7696 - val_acc: 0.6742\n",
      "Epoch 16/20\n",
      "328/328 [==============================] - 0s - loss: 1.7607 - acc: 0.7317 - val_loss: 1.7679 - val_acc: 0.6667\n",
      "Epoch 17/20\n",
      "328/328 [==============================] - 0s - loss: 1.7586 - acc: 0.7043 - val_loss: 1.7660 - val_acc: 0.6667\n",
      "Epoch 18/20\n",
      "328/328 [==============================] - 0s - loss: 1.7545 - acc: 0.7439 - val_loss: 1.7642 - val_acc: 0.6667\n",
      "Epoch 19/20\n",
      "328/328 [==============================] - 0s - loss: 1.7566 - acc: 0.6921 - val_loss: 1.7623 - val_acc: 0.6667\n",
      "Epoch 20/20\n",
      "328/328 [==============================] - 0s - loss: 1.7485 - acc: 0.7500 - val_loss: 1.7603 - val_acc: 0.6742\n",
      "132/132 [==============================] - 0s\n",
      "Test score: 1.7603045702\n",
      "Test accuracy: 0.674242436886\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(30000,256, dropout=0.2))\n",
    "model.add(LSTM(8, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(nb_word_class))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train, batch_size=328, nb_epoch=20,\n",
    "         validation_data=(x_test, y_test),shuffle=False)\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=328)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sandra went back to the bedroom', 'colin didnt like fish tacos', 'dick got a job', 'timmys room is a mess', 'she found her phone in the car', 'sorry thats not it', 'gina loved her grandma', 'anns fridge was broken', 'ann and tim loved astrology', 'sam quits her job', 'correct', 'they had a great tea party', 'tony was scared of the ocean', 'yes that is correct', 'sandra grabbed the milk there', 'colin drinks beer at the bar', 'yes thats right', 'mary put down the apple', 'john went to the garden', 'tom and dick loved their trucks', 'tony enjoyed going on the cruise', 'neil played hockey', 'where is the football', 'larry became careful', 'daniel left the apple', 'scott gave him a high five', 'mary travelled to the bathroom', 'sarah decided to move to europe', 'mary moved to the kitchen', 'his momther was concerned', 'mary got the football there', 'don hated elections', 'jason took a hot shower', 'dick didnt like theme parks', 'gary was a brave child', 'ann ate an apple', 'tim was entering a baking contest', 'neil was visiting ireland', 'jim made spinich cookies', 'daniel travelled to the office', 'no that is incorrect', 'john and billy were disappointed', 'neil enjoyed ireland', 'the game was canceled', 'he is happy now', 'where is mary', 'bobby had a good party', 'sorry wrong', 'neils teacher loved the gift', 'the company fixed his television', 'the food was amazing', 'dick agreed', 'my daughter was excited', 'sam loved his old belt', 'he sold them for five dollars', 'larry bought a new motocycle', 'everyone bought gifts', 'sandra put down the football there', 'dick had a great time at the park', 'sam went on a diet', 'where is daniel', 'rachel adpoted a cat over ten years', 'she went outside to fly a kite', 'danny bought a boat', 'daniel journeyed to the hallway', 'she became a better cook', 'neil wanted to play hockey', 'the two women fell off a cliff', 'tim won the baking contest', 'he found a large spider']\n"
     ]
    }
   ],
   "source": [
    "reverse_train_sentences = list(train_sentences)\n",
    "reverse_train_sentences.reverse()\n",
    "\n",
    "random_train_sentences = list(train_sentences)\n",
    "random.shuffle(random_train_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
