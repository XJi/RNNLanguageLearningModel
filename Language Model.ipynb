{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)\n",
    "nb_word_class = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_sentence(s):\n",
    "    c = s.lower().strip()\n",
    "    return re.sub('[^a-z ]', '', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_word_class(tag):\n",
    "    if tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return 1 #'adjective'\n",
    "    if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return 2 #'noun'\n",
    "    if tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return 3 #'verb'\n",
    "    if tag in ['CC', 'IN']:\n",
    "        return 4 #'link'\n",
    "    return 0 #'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_tagging(sentence):\n",
    "    translation = list()\n",
    "    for word, tag in sentence:\n",
    "        translation.append((word, translate_word_class(tag)))\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_encoding(sentence):\n",
    "    encoded = list()\n",
    "    for word, tag in sentence:\n",
    "        encodedInt = one_hot(word,30000)[0]\n",
    "        encoded.append([encodedInt,tag])\n",
    "    return encoded        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "END = 6\n",
    "def sentence_labeling(sentence):\n",
    "    labels = list()\n",
    "    for word, tag in sentence[1:]:\n",
    "        labels.append(tag)\n",
    "    labels.append(END)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag_words(sentences):\n",
    "    tagged_words = list()\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        tagged_words.append(nltk.pos_tag(words))\n",
    "    return tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Ymatrix(numbers,size,var_index):\n",
    "    matrix = np.zeros((size))\n",
    "    i = 0\n",
    "    for number in numbers:\n",
    "        matrix[i] = number[var_index]\n",
    "        i=i+1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_Xmatrix(numbers,size,var_index):\n",
    "    matrix = np.zeros((size,1))\n",
    "    sub_matrix = np.zeros((1))\n",
    "    i = 0\n",
    "    for number in numbers:\n",
    "        sub_matrix[0] = number[var_index]\n",
    "        matrix[i][0] = sub_matrix\n",
    "        i=i+1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sentences = list()\n",
    "with open('./data_set/training_set70.txt') as train:\n",
    "    for line in train:\n",
    "        train_sentences.append(clean_sentence(line))\n",
    "\n",
    "test_sentences = list()\n",
    "with open('./data_set/test_set25.txt') as train:\n",
    "    for line in train:\n",
    "        test_sentences.append(clean_sentence(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['correct', 'dick agreed', 'sorry wrong', 'where is mary', 'he is happy now']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagged_sentences = tag_words(train_sentences)\n",
    "tagged_sentences_test = tag_words(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('sam', 'JJ'), ('didnt', 'NNS'), ('like', 'IN'), ('elections', 'NNS')],\n",
       " [('dick', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('sam', 'NN'),\n",
       "  ('were', 'VBD'),\n",
       "  ('disappointed', 'JJ')],\n",
       " [('where', 'WRB'), ('is', 'VBZ'), ('my', 'PRP$'), ('apple', 'NN')],\n",
       " [('yes', 'RB'), ('thats', 'NNS'), ('correct', 'VBP')],\n",
       " [('no', 'DT'), ('its', 'PRP$'), ('wrong', 'NN')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[:5]\n",
    "tagged_sentences_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tagged_sentences = list(map(map_tagging, tagged_sentences))\n",
    "test_tagged_sentences = list(map(map_tagging, tagged_sentences_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('correct', 2)],\n",
       " [('dick', 2), ('agreed', 3)],\n",
       " [('sorry', 2), ('wrong', 1)],\n",
       " [('where', 0), ('is', 3), ('mary', 1)],\n",
       " [('he', 0), ('is', 3), ('happy', 1), ('now', 0)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tagged_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = flatten(map(map_encoding, my_tagged_sentences))\n",
    "test = flatten(map(map_encoding, test_tagged_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (328, 1)\n",
      "X_test shape: (132, 1)\n",
      "y_train shape: (328, 5)\n",
      "y_test shape: (132, 5)\n",
      "[[ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]]\n",
      "[[  2.64960000e+04]\n",
      " [  4.15300000e+03]\n",
      " [  2.30400000e+04]\n",
      " [  1.51660000e+04]\n",
      " [  8.65200000e+03]\n",
      " [  1.89720000e+04]\n",
      " [  2.64960000e+04]\n",
      " [  7.28100000e+03]\n",
      " [  1.53210000e+04]\n",
      " [  8.49900000e+03]\n",
      " [  2.75040000e+04]\n",
      " [  1.75770000e+04]\n",
      " [  2.21400000e+04]\n",
      " [  2.72070000e+04]\n",
      " [  5.60800000e+03]\n",
      " [  2.80290000e+04]\n",
      " [  1.07830000e+04]\n",
      " [  2.04300000e+03]\n",
      " [  4.54800000e+03]\n",
      " [  7.00400000e+03]\n",
      " [  2.25200000e+03]\n",
      " [  5.40300000e+03]\n",
      " [  8.04800000e+03]\n",
      " [  2.24840000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.26850000e+04]\n",
      " [  7.52500000e+03]\n",
      " [  8.22000000e+02]\n",
      " [  4.15300000e+03]\n",
      " [  2.94270000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.06150000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.23950000e+04]\n",
      " [  1.10110000e+04]\n",
      " [  2.65100000e+04]\n",
      " [  2.90600000e+03]\n",
      " [  2.75040000e+04]\n",
      " [  2.38360000e+04]\n",
      " [  2.77570000e+04]\n",
      " [  2.69660000e+04]\n",
      " [  7.15900000e+03]\n",
      " [  2.35530000e+04]\n",
      " [  2.21400000e+04]\n",
      " [  1.12590000e+04]\n",
      " [  1.89720000e+04]\n",
      " [  3.15800000e+03]\n",
      " [  8.73800000e+03]\n",
      " [  1.43580000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  1.12460000e+04]\n",
      " [  2.64960000e+04]\n",
      " [  7.45800000e+03]\n",
      " [  5.40300000e+03]\n",
      " [  6.00000000e+00]\n",
      " [  1.12590000e+04]\n",
      " [  5.78300000e+03]\n",
      " [  1.43580000e+04]\n",
      " [  1.35240000e+04]\n",
      " [  9.11000000e+03]\n",
      " [  5.40300000e+03]\n",
      " [  3.61300000e+03]\n",
      " [  2.18700000e+04]\n",
      " [  1.10110000e+04]\n",
      " [  1.63790000e+04]\n",
      " [  1.89720000e+04]\n",
      " [  1.83300000e+04]\n",
      " [  2.64960000e+04]\n",
      " [  1.25250000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  1.75410000e+04]\n",
      " [  1.14500000e+03]\n",
      " [  4.15300000e+03]\n",
      " [  2.34750000e+04]\n",
      " [  4.59300000e+03]\n",
      " [  1.48900000e+04]\n",
      " [  2.87780000e+04]\n",
      " [  1.21080000e+04]\n",
      " [  2.37360000e+04]\n",
      " [  1.19910000e+04]\n",
      " [  9.11000000e+03]\n",
      " [  1.98080000e+04]\n",
      " [  6.76500000e+03]\n",
      " [  7.95700000e+03]\n",
      " [  2.03460000e+04]\n",
      " [  2.15070000e+04]\n",
      " [  1.41950000e+04]\n",
      " [  1.10110000e+04]\n",
      " [  1.63790000e+04]\n",
      " [  2.24840000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.17450000e+04]\n",
      " [  2.64960000e+04]\n",
      " [  1.89720000e+04]\n",
      " [  1.41950000e+04]\n",
      " [  8.73800000e+03]\n",
      " [  1.43580000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.73150000e+04]\n",
      " [  1.33520000e+04]\n",
      " [  1.81520000e+04]\n",
      " [  1.51400000e+03]\n",
      " [  9.11000000e+03]\n",
      " [  4.59300000e+03]\n",
      " [  2.71730000e+04]\n",
      " [  1.81520000e+04]\n",
      " [  8.73800000e+03]\n",
      " [  2.70170000e+04]\n",
      " [  1.43580000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.07460000e+04]\n",
      " [  1.12590000e+04]\n",
      " [  2.43930000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  1.55750000e+04]\n",
      " [  2.24840000e+04]\n",
      " [  2.01690000e+04]\n",
      " [  2.72650000e+04]\n",
      " [  1.81520000e+04]\n",
      " [  1.89720000e+04]\n",
      " [  2.64960000e+04]\n",
      " [  7.95700000e+03]\n",
      " [  9.04100000e+03]\n",
      " [  1.60140000e+04]\n",
      " [  2.91290000e+04]\n",
      " [  2.65980000e+04]\n",
      " [  2.90600000e+03]\n",
      " [  5.78300000e+03]\n",
      " [  1.43580000e+04]\n",
      " [  2.50870000e+04]\n",
      " [  1.43580000e+04]\n",
      " [  1.15180000e+04]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/keras/utils/np_utils.py:23: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    }
   ],
   "source": [
    "x_train = create_Xmatrix(x,len(x),0)\n",
    "\"y_train = create_Ymatrix(x,len(x),1)\"\n",
    "y_train = to_categorical(create_Ymatrix(x,len(x),1),nb_word_class)\n",
    "x_test = create_Xmatrix(test,len(test),0)\n",
    "y_test = to_categorical(create_Ymatrix(test,len(test),1),nb_word_class)\n",
    "\n",
    "print('X_train shape:', x_train.shape)\n",
    "print('X_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(y_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 328 samples, validate on 132 samples\n",
      "Epoch 1/10\n",
      "328/328 [==============================] - 1s - loss: 1.6089 - acc: 0.2683 - val_loss: 1.6078 - val_acc: 0.3864\n",
      "Epoch 2/10\n",
      "328/328 [==============================] - 0s - loss: 1.6073 - acc: 0.3720 - val_loss: 1.6060 - val_acc: 0.4545\n",
      "Epoch 3/10\n",
      "328/328 [==============================] - 0s - loss: 1.6042 - acc: 0.5091 - val_loss: 1.6042 - val_acc: 0.5152\n",
      "Epoch 4/10\n",
      "328/328 [==============================] - 0s - loss: 1.6012 - acc: 0.5854 - val_loss: 1.6022 - val_acc: 0.5833\n",
      "Epoch 5/10\n",
      "328/328 [==============================] - 0s - loss: 1.5989 - acc: 0.6250 - val_loss: 1.6003 - val_acc: 0.6288\n",
      "Epoch 6/10\n",
      "328/328 [==============================] - 0s - loss: 1.5970 - acc: 0.6524 - val_loss: 1.5984 - val_acc: 0.6591\n",
      "Epoch 7/10\n",
      "328/328 [==============================] - 0s - loss: 1.5937 - acc: 0.7287 - val_loss: 1.5965 - val_acc: 0.6742\n",
      "Epoch 8/10\n",
      "328/328 [==============================] - 0s - loss: 1.5910 - acc: 0.7439 - val_loss: 1.5944 - val_acc: 0.7348\n",
      "Epoch 9/10\n",
      "328/328 [==============================] - 0s - loss: 1.5884 - acc: 0.7530 - val_loss: 1.5924 - val_acc: 0.7424\n",
      "Epoch 10/10\n",
      "328/328 [==============================] - 0s - loss: 1.5887 - acc: 0.7957 - val_loss: 1.5903 - val_acc: 0.7500\n",
      "132/132 [==============================] - 0s\n",
      "Test score: 1.59028768539\n",
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(30000,256, dropout=0.2))\n",
    "model.add(LSTM(8, dropout_W=0.2, dropout_U=0.2))  # try using a GRU instead, for fun\n",
    "model.add(Dense(nb_word_class))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train, batch_size=328, nb_epoch=10,\n",
    "         validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=328)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
